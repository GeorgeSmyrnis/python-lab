{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to  Machine learning with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from [glouppe's tutorial](https://github.com/glouppe/tutorials-scikit-learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit-learn is the most popular machine learning in Python.\n",
    "* Simple and efficient algorithm implementations\n",
    "* Implements a wide variety of well-established machine learning algorithms\n",
    "* Has extensive <a href=\"http://scikit-learn.org/dev/documentation.html\">documentation</a> \n",
    "* Lots of available <a href=\"http://scikit-learn.org/dev/auto_examples/index.html\">examples</a>\n",
    "* Project is mature and stable and follows srict development guidelines \n",
    "* Builds upon NumPy and SciPy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Supervised learning:__\n",
    "\n",
    "* Linear models (Ridge, Lasso, Elastic Net, ...)\n",
    "* Support Vector Machines\n",
    "* Tree-based methods (Random Forests, Bagging, GBRT, ...)\n",
    "* Nearest neighbors \n",
    "* Neural networks (basics)\n",
    "* Gaussian Processes\n",
    "* Feature selection\n",
    "\n",
    "__Unsupervised learning:__\n",
    "\n",
    "* Clustering (KMeans, Ward, ...)\n",
    "* Matrix decomposition (PCA, ICA, ...)\n",
    "* Density estimation\n",
    "* Outlier detection\n",
    "\n",
    "__Model selection and evaluation:__\n",
    "\n",
    "* Cross-validation\n",
    "* Grid-search\n",
    "* Lots of metrics\n",
    "\n",
    "_... and more!_ (See [Reference](http://scikit-learn.org/dev/modules/classes.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "\n",
    "\n",
    "All learning algorithms in scikit-learn share a uniform and limited API consisting of complementary interfaces:\n",
    "\n",
    "- an `estimator` interface for building and fitting models;\n",
    "- a `predictor` interface for making predictions;\n",
    "- a `transformer` interface for converting data.\n",
    "\n",
    "Goal: enforce a simple and consistent API to __make it trivial to swap or plug algorithms__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators\n",
    "\n",
    "The basic abstraction in sklearn is an estimator. Estimators implement the `fit(X, y=None)` method that fits the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors\n",
    "\n",
    "Predictors use a fitted model to make predictions on data. They implement the `predict(X)` method that returns the predicted `y` values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "Transformers change the data to new representations. They implement the `transform(X, y=None)` method that returns the modified data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Classification Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comes as a finite learning set ${\\cal L} = (X, y)$ where\n",
    "* Input samples are given as an array $X$ of shape `n_samples` $\\times$ `n_features`, taking their values in ${\\cal X}$;\n",
    "* Output values are given as an array $y$, taking _symbolic_ values in ${\\cal Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of supervised classification is to build an estimator $\\varphi: {\\cal X} \\mapsto {\\cal Y}$ minimizing\n",
    "\n",
    "$$\n",
    "Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}\n",
    "$$\n",
    "\n",
    "where $\\ell$ is a loss function, e.g., the zero-one loss for classification $\\ell_{01}(Y,\\hat{Y}) = 1(Y \\neq \\hat{Y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data loading and inspection\n",
    "\n",
    "- Input data => Numpy arrays or Scipy sparse matrices ;\n",
    "- $X$ => Data samples. Shape  => `n_samples` $\\times$ `n_features`\n",
    "- $y$ => Labels. Shape => `n_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pprint\n",
    "\n",
    "bc_data = load_breast_cancer()\n",
    "X, y = bc_data.data, bc_data.target\n",
    "\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "print(\"X shape: (n_samples) {} x (n_features) {}\"\n",
    "      .format(X.shape[0], X.shape[1]))\n",
    "print(\"y shape: (n_samples) {}\".format(y.shape[0]))\n",
    "\n",
    "# set converts a list into a set containing the unique elements in the list\n",
    "print(\"Unique labels: {}\".format(set(y)))\n",
    "print(\"Unique labels: {}\".format(bc_data.target_names))\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "print(\"Features:\")\n",
    "pprint.pprint(bc_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice way to gather intuition about the data is to plot the features against each other.\n",
    "\n",
    "This shows how good we can separate the classes based on the features\n",
    "\n",
    "First let's define some plotting utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def plot_features(X, feature_idx, feature_names, labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    zeros = ax.scatter(\n",
    "        X[y == 0, feature_idx[0]],\n",
    "        X[y == 0, feature_idx[1]], \n",
    "        label=labels[0], c='orange', alpha=0.3, s=100)\n",
    "    ones = ax.scatter(\n",
    "        X[y == 1, feature_idx[0]], \n",
    "        X[y == 1, feature_idx[1]], \n",
    "        label=labels[1], c='b', alpha=0.3, s=100)\n",
    "    plt.title(\"{} vs {}\".format(feature_names[0], feature_names[1]))\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Observe the following plots.\n",
    "\n",
    "1. What conclusion can you draw from the first plot about the relation between mean radius and perimeter?\n",
    "2. If you were to select only 2 features for classification, which of the following pairs would you prefer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(\n",
    "    X, [0, 2],\n",
    "    [bc_data.feature_names[0], bc_data.feature_names[2]],\n",
    "    bc_data.target_names)\n",
    "plot_features(\n",
    "    X, [2, 21],\n",
    "    [bc_data.feature_names[2], bc_data.feature_names[21]],\n",
    "    bc_data.target_names)\n",
    "plot_features(\n",
    "    X, [2, 6],\n",
    "    [bc_data.feature_names[2], bc_data.feature_names[6]],\n",
    "    bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Transformers to process our data before classifying them to obtain more convenient representations.\n",
    "\n",
    "A common preprossecing step Z-normalization, where we normalize data to have zero mean and unit std using the transformation\n",
    "\n",
    "\n",
    "$$\n",
    "x_{new} = \\frac{x_{old} - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement Z-normalization as follows:\n",
    "\n",
    "__Question__: Compare the scaled features plot with the original. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def znorm(x, axis=0):\n",
    "    return (x - x.mean(axis=axis)) / x.std(axis=axis)\n",
    "\n",
    "X_scaled_custom = znorm(X)\n",
    "\n",
    "plot_features(\n",
    "    X_scaled_custom, [2, 6],\n",
    "    [\"{} scaled\".format(bc_data.feature_names[2]),\n",
    "     \"{} scaled\".format(bc_data.feature_names[6])],\n",
    "    bc_data.target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already implemented as a Transformer in sklearn under the name of `StandardScaler`.\n",
    "From now on we will use the Transformer version, as it can be  combined seamlessly with other sklearn utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "plot_features(\n",
    "    X_scaled, [2, 6],\n",
    "    [\"{} scaled\".format(bc_data.feature_names[2]),\n",
    "     \"{} scaled\".format(bc_data.feature_names[6])],\n",
    "    bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can verify `znorm` has the same result as the `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isclose can be used for floating point comparison\n",
    "print(np.all(np.isclose(X_scaled, X_scaled_custom, rtol=1e-6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we will use only the mean perimeter and mean concavity as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_scaled[:, [2, 6]]\n",
    "\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define some utilities to visualize the classifier's __decision function__.\n",
    "\n",
    "This shows us where the classifier decides to separate the data into the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_clf(clf, X, y, labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    # title for the plots\n",
    "    title = ('Decision surface of Classifier')\n",
    "    # Set-up grid for plotting.\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    \n",
    "    x_min, x_max = X0.min() - 1, X0.max() + 1\n",
    "    y_min, y_max = X1.min() - 1, X1.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, .05),\n",
    "                         np.arange(y_min, y_max, .05))\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    \n",
    "    zeros = ax.scatter(\n",
    "        X0[y == 0], X1[y == 0],\n",
    "        c='blue', label=labels[0],\n",
    "        s=60, alpha=0.9, edgecolors='k')\n",
    "    ones = ax.scatter(\n",
    "        X0[y == 1], X1[y == 1],\n",
    "        c='red', label=labels[1], \n",
    "        s=60, alpha=0.9, edgecolors='k')\n",
    "    \n",
    "    ax.set_ylabel(labels[1])\n",
    "    ax.set_xlabel(labels[0])\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to perform the classification.\n",
    "\n",
    "We can fit a Naive Bayes model to classify the data in the 2 classes. Naive Bayes comes already implemented in sklearn in the `GaussianNB` class. `GaussianNB` assumes the data are normally distributed and implements the Estimator / Predictor interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_new[17:27])\n",
    "\n",
    "y_true = y[17:27]\n",
    "\n",
    "print(\"Naive Bayes - Predicted labels:\\t {}\".format(y_pred))\n",
    "print(\"True labels:\\t\\t\\t {}\".format(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fit a Gaussian Mixture Model (GMM) classifier with a diagonal covariance matrix. This can be done using the `GaussianMixture` classifier.\n",
    "\n",
    "`GaussianMixture` implements the Estimator / Predictor interfaces.\n",
    "\n",
    "__Question__: Try changing `n_init=1` in the following code snippets. Run the cell a few times. What do you observe? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "clf = GaussianMixture(n_components=2, covariance_type='diag', init_params='random', n_init=10, max_iter=20)\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with a full covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "clf = GaussianMixture(n_components=2, covariance_type='full', init_params='random', n_init=100, max_iter=20)\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fit a Logistic Regression (LR) model. LR tries to model the decision boundary as an hyperplane.\n",
    "\n",
    "LR implements the Estimator / Predictor interfaces.\n",
    "\n",
    "LR is a __linear classifier__, meaning it separates data by drawing a hyperplane between the two classes.\n",
    "Observe how LR tries to separate data by drawing a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) try to find the hyperplane which has the largest distance to the nearest training points of any class. In sklearn SVMs are implemented in the `SVC` class.\n",
    "\n",
    "Again `SVC` implements the Estimator / Predictor interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\")  # try kernel=\"rbf\" instead\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR and SVM both try to separate data by drawing a line / hyperplane. Such classifiers are called `linear classifiers`.\n",
    "\n",
    "We can modify SVM to first project the data using a `kernel`. You will learn about this during the semester.\n",
    "\n",
    "The important thing here is that with kernels we can get non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"rbf\")  # try kernel=\"rbf\" instead\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to obtain a non-linear classifier is to use multilayer neural networks. In sklearn neural networks are implemented in the `MLPClassifier` class.\n",
    "\n",
    "__Question__: Which interfaces does `MLPClassifier` implement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit:\n",
    "- A small NN (2 layers with 50 neurons each)\n",
    "- A medium NN (3 layers with 100 neurons each)\n",
    "- A large NN (4 layers 1000 neurons each)\n",
    "\n",
    "__Question__: Observe the decision boundaries. Which architecture would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50, 50),\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    learning_rate='invscaling')\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100),\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    learning_rate='invscaling')\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(1000, 1000, 1000, 1000),\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    learning_rate='invscaling')\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some metrics to compare which model performs best. Here we will examine\n",
    "\n",
    "- The error on the training data\n",
    "- The error on the test data\n",
    "\n",
    "__Question__: What does `performs best` mean for a classifier? Given this which of the previous metrics is more reliable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error on training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import zero_one_loss\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_new, y)\n",
    "print(\"Training error =\", zero_one_loss(y, clf.predict(X_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error on test data\n",
    "\n",
    "Issue: the training error is a __biased__ estimate of the generalization error.\n",
    "\n",
    "Solution: Divide ${\\cal L}$ into two disjoint parts called training and test sets (usually using 70% for training and 30% for test).\n",
    "- Use the training set for fitting the model;\n",
    "- Use the test set for evaluation only, thereby yielding an unbiased estimate.\n",
    "\n",
    "__The model should never see samples from the test dataset during training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, random_state=2)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training error =\", zero_one_loss(y_train, clf.predict(X_train)))\n",
    "print(\"Test error =\", zero_one_loss(y_test, clf.predict(X_test)))\n",
    "\n",
    "plot_clf(clf, X_test, y_test, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: Beware of bias when you estimate model performance:\n",
    "- Training score is often an optimistic estimate of the true performance;\n",
    "- __The same data should not be used both for training and evaluation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Try changing `random_state` in the `train_test_split` arguments. What do you observe? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: \n",
    "- When ${\\cal L}$ is small, training on 70% of the data may lead to a model that is significantly different from a model that would have been learned on the entire set ${\\cal L}$. \n",
    "- Yet, increasing the size of the training set (resp. decreasing the size of the test set), might lead to an inaccurate estimate of the generalization error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: K-Fold cross-validation. \n",
    "- Split ${\\cal L}$ into K small disjoint folds. \n",
    "- Train on K-1 folds, evaluate the test error one the held-out fold.\n",
    "- Repeat for all combinations and average the K estimates of the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), X_new, y, \n",
    "                         cv=KFold(n_splits=5, random_state=42), \n",
    "                         scoring=\"accuracy\")\n",
    "print(\"CV error = %f +-%f\" % (1. - np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Try changing the `random_state` in the above example and observe the CV error. What do you observe? Can we be confident about the CV error estimate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy score__\n",
    "\n",
    "When evaluating the models we want to report some standard metrics.\n",
    "\n",
    "The simplest metric for classification is the __accuracy score__, which is the default metric in most models.\n",
    "\n",
    "The accuracy score is simply the percentage of the correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score is classifiers default score. clf.score: {}\"\n",
    "      .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / y_true.shape[0]\n",
    "\n",
    "print(\"We can implement it very easily. acc: {}\"\n",
    "      .format(acc(y_test, clf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"But it's already implemented in sklearn. accuracy_score: {}\"\n",
    "      .format(accuracy_score(y_test, clf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Precision, recall and F1 score__\n",
    "\n",
    "In many cases accuracy is simply not enough to evaluate a classifier. Instead we may need to know how many samples get incorrectly classified as positive (the False Positives $FP$) and how many get classified incorrectly as negative (the False Negatives $FN$).\n",
    "\n",
    "For example in the malignant vs benign breast cancer we are studying, the consequences are far larger if we missclassify a malignant tumor as benign than if we missclassify a benign tumor as malignant.\n",
    "\n",
    "Here we study the following metrics:\n",
    "\n",
    "- __precision__ $P$: the fraction of relevant instances among the retrieved instances\n",
    "- __recall__ $R$: the fraction of relevant instances that have been retrieved over the total amount of relevant instances\n",
    "- __F1__: The harmonic mean of $P$ and $R$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$R = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F1 = \\frac{2 P R}{P + R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Precision =\", precision_score(y_test, clf.predict(X_test)))\n",
    "print(\"Recall =\", recall_score(y_test, clf.predict(X_test)))\n",
    "print(\"F1 =\", f1_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "An even more fine grained metric is the confusion matrix, which shows the number of samples that get missclassified in each class.\n",
    "\n",
    "Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).[2] The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the confusion matrix as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n",
    "plot_confusion_matrix(cm, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: How many malignant tumors did we misdiagnose as benign?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many points to be covered for model selection, in this tutorial we will consider the following:\n",
    "\n",
    "1. Hyperparameter search\n",
    "2. Validation curves\n",
    "3. Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search\n",
    "\n",
    "Hyperparameter search is the problem of finding the best parameters for the model to fit the best performing model.\n",
    "\n",
    "The simplest solution is to create a grid of all possible parameters and evaluate the model with each set of parameters in the grid, i.e. to perform a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def grid_search(clf, param_grid, X, y, cv=5, scoring=\"f1\"):\n",
    "    parameters = sorted(param_grid)\n",
    "    combinations = itertools.product(*(param_grid[p] for p in parameters))\n",
    "    best_score = -np.Inf\n",
    "    best_params = {}\n",
    "    for param_set in combinations:\n",
    "        kwargs = dict(zip(parameters, param_set))\n",
    "        clf.set_params(**kwargs)\n",
    "        scores = cross_val_score(clf, X, y, cv=cv, scoring=scoring)\n",
    "        score = np.mean(scores)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = param_set\n",
    "    return best_score, dict(zip(parameters, best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Does `grid_search` work for every classifier in sklearn? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_params = grid_search(\n",
    "    LogisticRegression(),\n",
    "    {\"C\": np.linspace(0.05, 5, 50), \"penalty\": [\"l1\", \"l2\"]},\n",
    "    X_new, y,\n",
    "    scoring=\"f1\",\n",
    "    cv=5)\n",
    "\n",
    "print(\"Best score = {}, Best parameters = {}\".format(best_score, \n",
    "                                                     best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Should you report the best score as an estimate of the generalization error of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn implements grid search in `GridSearchCV` class as an Estimator. One obvious improvement is that it has parallelization support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Note in param_grid we pass the names of the parameters we want to search\n",
    "# as they appear in the classifier's class arguments and the possible values\n",
    "# they can take\n",
    "grid = GridSearchCV(LogisticRegression(),\n",
    "                    param_grid={\"C\": np.linspace(0.05, 5, 50),\n",
    "                                \"penalty\": [\"l1\", \"l2\"]},\n",
    "                    scoring=\"f1\",\n",
    "                    cv=5, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_new, y)\n",
    "\n",
    "print(\"Best score = {}, Best parameters = {}\".format(grid.best_score_, \n",
    "                                                     grid.best_params_))\n",
    "\n",
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `grid.best_score_` is not independent from the best model, since its construction was guided by the optimization of this quantity. \n",
    "\n",
    "- As a result, the optimized `grid.best_score_` estimate _may_ in fact be a biased, optimistic, estimate of the true performance of the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__: Use __nested__ cross-validation for correctly selecting the model __and__ correctly evaluating its performance (or use a validation set if you have a lot of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(),\n",
    "                    param_grid={\"C\": np.linspace(0.05, 5, 50), \n",
    "                                \"penalty\": [\"l1\", \"l2\"]},\n",
    "                    scoring=\"f1\",\n",
    "                    cv=5, n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(grid, X_new, y, cv=5, scoring=\"f1\")\n",
    "\n",
    "# Unbiased estimate of the accuracy\n",
    "print(\"{} +-{}\".format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(**best_params).fit(X_train, y_train)\n",
    "plot_clf(clf, X_test, y_test, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important visualization is the __validation curve__. This plot shows how a metric changes with respect to a model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "train_scores, test_scores = validation_curve(LogisticRegression(), X_new, y, \"C\",\n",
    "                                             np.linspace(0.001, 5, 100),\n",
    "                                             scoring='f1',\n",
    "                                             cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_validation_curve(train_scores, test_scores, param_name, param_values, ylim=(0, 1)):\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.title(\"Validation Curve\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(ylim[0], ylim[1])\n",
    "    lw = 2\n",
    "    plt.plot(param_values, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\")\n",
    "    plt.fill_between(param_values, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                     color=\"darkorange\")\n",
    "    plt.plot(param_values, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\")\n",
    "    plt.fill_between(param_values, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                     color=\"navy\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_validation_curve(train_scores, test_scores, \"C\", np.linspace(0.001, 5, 100), ylim=[.88, .99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Given a validation curve like this, is it worth spending time and resources to perform further tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "\n",
    "Learning curves plot the test score and the training score as we vary the available training data.\n",
    "\n",
    "Learning curves are a very useful tool to interpret a model's behavior, if you know how to read them. \n",
    "\n",
    "[Here's a nice article](https://www.dataquest.io/blog/learning-curves-machine-learning/) describing all the tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    LogisticRegression(), X_new, y, cv=5, n_jobs=-1, \n",
    "    train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_learning_curve(train_scores, test_scores, train_sizes, ylim=(0, 1)):\n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve\")\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "plot_learning_curve(train_scores, test_scores, train_sizes, ylim=(.6, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Is adding more data to the model going to improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn introduces the concept of pipelines. Pipelines not only make the code short and easy to understand, but also make it easy to ensure that everything runs on the correct set of data.\n",
    "\n",
    "For example in the above code we have a serious bug. When we preprocessed data with StandardScaler, we used the entire dataset to fit them, not the training dataset.\n",
    "\n",
    "__Incorrect preprocessing is another way to introduce bias to the models__\n",
    "\n",
    "or better\n",
    "\n",
    "__Incorrect preprocessing is the most common way to introduce bias to the models__\n",
    "\n",
    "or even better\n",
    "\n",
    "__Incorrect preprocessing is the hardest to spot source of bias for the models__\n",
    "\n",
    "\n",
    "A correct pipeline should look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "bc_data = load_breast_cancer()\n",
    "X, y = bc_data.data, bc_data.target\n",
    "\n",
    "\n",
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('znorm', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'lr__C': np.linspace(0.01, 1, 30),\n",
    "    'lr__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    clf_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_scores = cross_val_score(grid_search,\n",
    "                              X[:, [2, 6]], y, \n",
    "                              cv=5, \n",
    "                              scoring=\"f1\")\n",
    "\n",
    "best_params = grid_search.fit(X, y).best_params_\n",
    "\n",
    "import pprint\n",
    "print(\"Best model params:\")\n",
    "pprint.pprint(best_params)\n",
    "print(\"Best score: {}+-{}\".format(np.mean(best_scores), \n",
    "                                  np.std(best_scores)))\n",
    "\n",
    "# flatten_best_params = {k.split('_', 2)[-1]: v for k, v in best_params.items()}\n",
    "\n",
    "clf_pipeline.set_params(**best_params)\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    clf_pipeline, X, y, cv=5, n_jobs=-1,  scoring='f1',\n",
    "    train_sizes=np.linspace(.1, 1.0, 5))\n",
    "\n",
    "plot_learning_curve(train_scores, test_scores, train_sizes, ylim=(.88, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
